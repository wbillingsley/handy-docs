{
    "docs": [
        {
            "location": "/", 
            "text": "Handy\n\n\nHandy is a small library of Scala classes to make asynchronous, lazy, reactive programming concise and easy for the web -- both in the browser (Scala.js) and on the server (Scala on the JVM). Rather than\nmake everything reactive (like other frameworks that use the observer pattern throughout), it tries to make reactive programming \nlazy\n -- only be reactive when it needs to be, and otherwise just look like\nfunction calls.\n\n\nIt also includes classes I find commonly useful for implementing web systems, for example typed IDs, and a simple asynchonrous permissions system. This makes it possible, for instance, to write\npermission logic that can be run transparently in the browser or on the server.\n\n\nPrograms using handy usually make heavy use of Scala's for-comprehension style. Suppose we want to organise a surprise birthday party for a bot called Algernon, from the bots it normally interacts with...\n\n\nval invitees:RefMany[Bot] = for {\n  canGetFriends \n- approval ask CanLookupFriends(Algernon)\n  friend \n- socialNetwork.friends(Algernon).take(10)\n  canMessage \n- approval ask CanSendNotificationTo(friend)\n  sent \n- invite(friend) if (sent.successful)\n} yield friend\n\n\n\n\nAnd we'll get an asynchronous list of those friends from Algernon's top 10 best friends who we were allowed to send an invite notification to. \n\n\nNote that this for-comprehension translates to a sequence of flatMap and map calls, and it can be asynchronous. For example, looking up the friends is probably an asynchonrous call. As, often, are the\npermissions checks.\n\n\nIts only dependency is on the interface of the Reactive-Streams standard, so that it can produce and consume reactive streams.\n\n\nWhat's the philosophy behind all this\n\n\nReally, the library was written from a philosophy of \"maximum indecisiveness\". \n\n\nI'd written a few apps that I'd needed to migrate between web frameworks, and between styles of database (SQL or NoSQL). And it irked me how much of the model of an app depended on the framework it used. For \nexample, who can do what is a business decision -- that should be part of the model code, not an annotation on an HTTP controller.\n\n\nSo I tried to write a little library that makes as few assumptions as possible, and lets decisions happen as late as possible. It is designed for programs that run on the Web -- either in the browser \nor on the server, so you can delay deciding where something will be calculated too. So it was designed through thinking fairly abstractly and unopinionatedly about programming for networked environments.\n\n\nNetwork programming in general\n\n\nWhen writing servers that need to talk to services and databases, or clients that need to talk to servers, we tend to load data only when we need it, and when we go to fetch it:\n\n\n\n\nit might take some time (asynchronous)\n\n\nit might fail\n\n\nit might not be there\n\n\n\n\nYou could describe that as a \nFuture[Option[T]]\n, but actually that'd be a bit painful because it's a nested type. Calling \nmap\n would let you work with the Option inside the Future, but then you'd need to call \nmap\n again to \nwork with the contained value. I'll let your imagination do the rest, but \nFuture[Option[T]]\n does not compose well if you're trying to write an algorithm more than a few steps long.\n\n\nInstead let's call our hypothetical type \nRef[T]\n, and say it has those properties. It \nmight\n be asynchronous. It \nmight\n fail. It \nmight\n produce nothing.\n\n\nInstead of backing this type with a \nFuture[Option[T]]\n, or some other specific structure, handy makes \nRef[T]\n a trait.\nSo that for instance we can have \nRefItself[T]\n if actually it turns out you've already got the object in your hands.\nThe principle here is about being lazy about converting anything. If you are writing an HTTP controller, then at the end of your algorithm, you will probably need to give your web framework a type it understands -- probably something like \nFuture[Result]\n.\nBut until the end, each step in your algorithm can just talk about a \nRef[This]\n and \nRef[That]\n.\n\n\nIn the case of a single value, that might seem a bit trivial, but this also lets us be very composable. Especially when we bring in \nRefMany[T]\n -- a reference to many things.\nA \nRefMany[T]\n could be an array of things. It could be a reactive stream. It could be an array of \nRef[T]\ns. It could be an array-of-refs-of-arrays-of-reactive-streams-of-futures. \nUntil we spit it out at the end, our algorithm doesn't have to care -- it's just a possibly-asynchronous reference to many things of type \nT\n. At the end, we'll usually decide to output the result as a \nreactive stream. But we can be lazy about that and only convert to a reactive stream when we need to -- along the way we can let the different kinds of \nRefMany[T]\n (those backed by arrays, or streams, \nor arrays of \nRef[T]\ns, etc) compose naturally.\n\n\nThe browser environment\n\n\nIn the browser, we don't usually want all the information about our app, just enough to display the UI and take actions as needed. Information gets loaded on demand. JavaScript is single-threaded, but calls\nto the server can involve significant latency.\n\n\nSo, we have an environment where we want to be:\n\n\n\n\nlazy \n\n\nasynchronous\n\n\nreactive (cache some information, but react to changes)\n\n\n\n\nBut there's not actually going to be a high degree of concurrency (JavaScript is a mostly single-threaded environment). \n\n\nThe server environment\n\n\nIn the server, we're usually responding to a request made by the client. Things happen asynchronously, because we may need to talk to databases and other systems. And we may need to fetch values along the way,\nthat might be used in several steps of the processing.\n\n\nSo, we have an environment where we want to be:\n\n\n\n\nlazy \n\n\nasynchronous\n\n\nreactive (because sometimes we have a websocket / persistent connection and want to stream out events)\n\n\n\n\nBut we tend not to share variables between requests (at least not mutable ones), so the individual variables within a request won't see a high degree of concurrency. The server itself will -- lots of requests\ngoing on at once, but they won't be sharing many variables.\n\n\nThe upshot of which is it's not too much of a stretch to make the same abstractions work in both environments.", 
            "title": "Home"
        }, 
        {
            "location": "/#handy", 
            "text": "Handy is a small library of Scala classes to make asynchronous, lazy, reactive programming concise and easy for the web -- both in the browser (Scala.js) and on the server (Scala on the JVM). Rather than\nmake everything reactive (like other frameworks that use the observer pattern throughout), it tries to make reactive programming  lazy  -- only be reactive when it needs to be, and otherwise just look like\nfunction calls.  It also includes classes I find commonly useful for implementing web systems, for example typed IDs, and a simple asynchonrous permissions system. This makes it possible, for instance, to write\npermission logic that can be run transparently in the browser or on the server.  Programs using handy usually make heavy use of Scala's for-comprehension style. Suppose we want to organise a surprise birthday party for a bot called Algernon, from the bots it normally interacts with...  val invitees:RefMany[Bot] = for {\n  canGetFriends  - approval ask CanLookupFriends(Algernon)\n  friend  - socialNetwork.friends(Algernon).take(10)\n  canMessage  - approval ask CanSendNotificationTo(friend)\n  sent  - invite(friend) if (sent.successful)\n} yield friend  And we'll get an asynchronous list of those friends from Algernon's top 10 best friends who we were allowed to send an invite notification to.   Note that this for-comprehension translates to a sequence of flatMap and map calls, and it can be asynchronous. For example, looking up the friends is probably an asynchonrous call. As, often, are the\npermissions checks.  Its only dependency is on the interface of the Reactive-Streams standard, so that it can produce and consume reactive streams.", 
            "title": "Handy"
        }, 
        {
            "location": "/#whats-the-philosophy-behind-all-this", 
            "text": "Really, the library was written from a philosophy of \"maximum indecisiveness\".   I'd written a few apps that I'd needed to migrate between web frameworks, and between styles of database (SQL or NoSQL). And it irked me how much of the model of an app depended on the framework it used. For \nexample, who can do what is a business decision -- that should be part of the model code, not an annotation on an HTTP controller.  So I tried to write a little library that makes as few assumptions as possible, and lets decisions happen as late as possible. It is designed for programs that run on the Web -- either in the browser \nor on the server, so you can delay deciding where something will be calculated too. So it was designed through thinking fairly abstractly and unopinionatedly about programming for networked environments.", 
            "title": "What's the philosophy behind all this"
        }, 
        {
            "location": "/#network-programming-in-general", 
            "text": "When writing servers that need to talk to services and databases, or clients that need to talk to servers, we tend to load data only when we need it, and when we go to fetch it:   it might take some time (asynchronous)  it might fail  it might not be there   You could describe that as a  Future[Option[T]] , but actually that'd be a bit painful because it's a nested type. Calling  map  would let you work with the Option inside the Future, but then you'd need to call  map  again to \nwork with the contained value. I'll let your imagination do the rest, but  Future[Option[T]]  does not compose well if you're trying to write an algorithm more than a few steps long.  Instead let's call our hypothetical type  Ref[T] , and say it has those properties. It  might  be asynchronous. It  might  fail. It  might  produce nothing.  Instead of backing this type with a  Future[Option[T]] , or some other specific structure, handy makes  Ref[T]  a trait.\nSo that for instance we can have  RefItself[T]  if actually it turns out you've already got the object in your hands.\nThe principle here is about being lazy about converting anything. If you are writing an HTTP controller, then at the end of your algorithm, you will probably need to give your web framework a type it understands -- probably something like  Future[Result] .\nBut until the end, each step in your algorithm can just talk about a  Ref[This]  and  Ref[That] .  In the case of a single value, that might seem a bit trivial, but this also lets us be very composable. Especially when we bring in  RefMany[T]  -- a reference to many things.\nA  RefMany[T]  could be an array of things. It could be a reactive stream. It could be an array of  Ref[T] s. It could be an array-of-refs-of-arrays-of-reactive-streams-of-futures. \nUntil we spit it out at the end, our algorithm doesn't have to care -- it's just a possibly-asynchronous reference to many things of type  T . At the end, we'll usually decide to output the result as a \nreactive stream. But we can be lazy about that and only convert to a reactive stream when we need to -- along the way we can let the different kinds of  RefMany[T]  (those backed by arrays, or streams, \nor arrays of  Ref[T] s, etc) compose naturally.", 
            "title": "Network programming in general"
        }, 
        {
            "location": "/#the-browser-environment", 
            "text": "In the browser, we don't usually want all the information about our app, just enough to display the UI and take actions as needed. Information gets loaded on demand. JavaScript is single-threaded, but calls\nto the server can involve significant latency.  So, we have an environment where we want to be:   lazy   asynchronous  reactive (cache some information, but react to changes)   But there's not actually going to be a high degree of concurrency (JavaScript is a mostly single-threaded environment).", 
            "title": "The browser environment"
        }, 
        {
            "location": "/#the-server-environment", 
            "text": "In the server, we're usually responding to a request made by the client. Things happen asynchronously, because we may need to talk to databases and other systems. And we may need to fetch values along the way,\nthat might be used in several steps of the processing.  So, we have an environment where we want to be:   lazy   asynchronous  reactive (because sometimes we have a websocket / persistent connection and want to stream out events)   But we tend not to share variables between requests (at least not mutable ones), so the individual variables within a request won't see a high degree of concurrency. The server itself will -- lots of requests\ngoing on at once, but they won't be sharing many variables.  The upshot of which is it's not too much of a stretch to make the same abstractions work in both environments.", 
            "title": "The server environment"
        }, 
        {
            "location": "/ref/", 
            "text": "Ref\n\n\nRef\n is a \"reference to something\". It is a monad that allows us to write code like this: \n\n\nfor {\n  page \n- LazyId(pId).of[Page]\n  approved \n- request.approval ask editPage(page.itself)\n  updated \n- update(page, json)\n  saved \n- pageDAO.save(updated)\n} yield saved\n\n\n\n\n...and not worry about which of those lines are producing \nOption\n, \nFuture\n, \nTry\n, or \nLazyId\n.\n\n\nEventually, it's equivalent to a \nFuture[Option]\n (and indeed has a \n.toFutOpt\n method) but a little more concise, \nand with a couple of extra features.\n\n\nRather than promote every type to \nFuture[Option]\n, it defines \nRef\n wrappers for each of the subtypes and composes \ntogether a structure. So, for example, if your chain of calls only involved \nOption\ns and \nTry\ns, your code will\nexecute synchronously on the same thread.\n\n\nComposing with RefMany\n\n\nRef\n and \nRefMany\n can be combined in for-comprehensions, and it'll get the plurality of the result right.\n\n\nfor { a \n- refA; b \n- refB } yield b           // Ref[B]\nfor { a \n- refA; b \n- refManyB } yield b       // RefMany[B]\nfor { a \n- refManyA; b \n- refB } yield b       // RefMany[B]\nfor { a \n- refManyA; b \n- refManyB } yield b   // RefMany[B]", 
            "title": "Ref"
        }, 
        {
            "location": "/ref/#ref", 
            "text": "Ref  is a \"reference to something\". It is a monad that allows us to write code like this:   for {\n  page  - LazyId(pId).of[Page]\n  approved  - request.approval ask editPage(page.itself)\n  updated  - update(page, json)\n  saved  - pageDAO.save(updated)\n} yield saved  ...and not worry about which of those lines are producing  Option ,  Future ,  Try , or  LazyId .  Eventually, it's equivalent to a  Future[Option]  (and indeed has a  .toFutOpt  method) but a little more concise, \nand with a couple of extra features.  Rather than promote every type to  Future[Option] , it defines  Ref  wrappers for each of the subtypes and composes \ntogether a structure. So, for example, if your chain of calls only involved  Option s and  Try s, your code will\nexecute synchronously on the same thread.", 
            "title": "Ref"
        }, 
        {
            "location": "/ref/#composing-with-refmany", 
            "text": "Ref  and  RefMany  can be combined in for-comprehensions, and it'll get the plurality of the result right.  for { a  - refA; b  - refB } yield b           // Ref[B]\nfor { a  - refA; b  - refManyB } yield b       // RefMany[B]\nfor { a  - refManyA; b  - refB } yield b       // RefMany[B]\nfor { a  - refManyA; b  - refManyB } yield b   // RefMany[B]", 
            "title": "Composing with RefMany"
        }, 
        {
            "location": "/refmany/", 
            "text": "RefMany\n\n\nRefMany\n is a reference to many of something. As with \nRef\n, it tries to unify across a few different common types\n\n\nRefMany\n might be asynchronous. They might fail to start. They might produce an error as they stream\nout elements. Eventually, RefMany is equivalent to a reactive stream. And again, there is a method to stream out the data as a \nreactive stream. But any conversion to a reactive stream happens as late as possible. So, for instance,\nif you were just to use \nSeq\ns, it would all happen synchronously on the same thread:\n\n\nval s1 = Seq(1, 2, 3).toRefMany\nval s2 = Seq(4, 5, 6).toRefMany\n\n(for {\n  a \n- s1\n  b \n- s2\n} yield a \n b).fold(_ + _) // Ref[Int], but computed synchronously\n\n\n\n\nIntroducing some asynchronous functions, it all still produces \nRefMany\n and \nRef\n but working \nasynchronously where needed\n\n\nval s1 = Seq(100, 200, 300).toRefMany\nval s2 = Seq(400, 500, 600).toRefMany\n\ndef getNthPrime(i:Int):Future[Int] = ???\n\n(for {\n  a \n- s1\n  athPrime \n- getNthPrime(a)\n  b \n- s2\n  bthPrime \n- getNthPrime(b)\n} yield a \n b).fold(_ + _) // Ref[Int], computed asynchronously\n\n\n\n\nThere are some calls that can cause your RefMany to be connected to a reactive stream early. For \nexample, if you have a \nRefTraversableRefMany\n, ie a sequence of possibly-asynchronous sequences,\nand call \ntake(5)\n the implementation will currently use a reactive stream to achieve this. \n\n\nComposing with Ref\n\n\nRef\n and \nRefMany\n compose together using Scala's for comprehensions\n\n\nfor {\n  foo \n- somethingReturningRefFoo()\n  bar \n- somethingReturningRefManyBar(foo)\n  baz \n- somethingReturningRefBaz(bar)\n} yield baz // RefMany[Baz]", 
            "title": "RefMany"
        }, 
        {
            "location": "/refmany/#refmany", 
            "text": "RefMany  is a reference to many of something. As with  Ref , it tries to unify across a few different common types  RefMany  might be asynchronous. They might fail to start. They might produce an error as they stream\nout elements. Eventually, RefMany is equivalent to a reactive stream. And again, there is a method to stream out the data as a \nreactive stream. But any conversion to a reactive stream happens as late as possible. So, for instance,\nif you were just to use  Seq s, it would all happen synchronously on the same thread:  val s1 = Seq(1, 2, 3).toRefMany\nval s2 = Seq(4, 5, 6).toRefMany\n\n(for {\n  a  - s1\n  b  - s2\n} yield a   b).fold(_ + _) // Ref[Int], but computed synchronously  Introducing some asynchronous functions, it all still produces  RefMany  and  Ref  but working \nasynchronously where needed  val s1 = Seq(100, 200, 300).toRefMany\nval s2 = Seq(400, 500, 600).toRefMany\n\ndef getNthPrime(i:Int):Future[Int] = ???\n\n(for {\n  a  - s1\n  athPrime  - getNthPrime(a)\n  b  - s2\n  bthPrime  - getNthPrime(b)\n} yield a   b).fold(_ + _) // Ref[Int], computed asynchronously  There are some calls that can cause your RefMany to be connected to a reactive stream early. For \nexample, if you have a  RefTraversableRefMany , ie a sequence of possibly-asynchronous sequences,\nand call  take(5)  the implementation will currently use a reactive stream to achieve this.", 
            "title": "RefMany"
        }, 
        {
            "location": "/refmany/#composing-with-ref", 
            "text": "Ref  and  RefMany  compose together using Scala's for comprehensions  for {\n  foo  - somethingReturningRefFoo()\n  bar  - somethingReturningRefManyBar(foo)\n  baz  - somethingReturningRefBaz(bar)\n} yield baz // RefMany[Baz]", 
            "title": "Composing with Ref"
        }, 
        {
            "location": "/latch/", 
            "text": "Latch\n\n\nA \nLatch\n is a little wrapper around a Scala \nFuture\n that makes it clearable. Think of it as a single item \nasynchronous cache. Latches can be dependent on one another, in what I call a \nlazy observer\n pattern. \n\n\nSuppose we start with a variable for who my closest friend is:\n\n\nval myClosestFriend = Latch.immediate(Algernon)\n\n\n\n\nAnd we have an asynchronous calculation that is dependent on it -- in this case to see if my closest friend interacts most with me on a social network.\n\n\nval closestFriendReciprocates:Latch[Boolean] = for { \n    friend \n- myClosestFriend \n    hisClosest \n- socialNetwork.getMostInteractedWithUser(friend)\n} yield (hisClosest == me)\n\n\n\n\nWe now have two latches that depend on each other. Suppose my closest friend changes for some reason. \n\n\nmyClosestFriend.fill(Bertie)\n\n\n\n\nThe variable on whether my closest friend reciprocates is now invalid. In an observer pattern, we might see that the closest friend has updated, and \nre-calculate the dependent value (make the call out to the social network). But actually, we might not need to use the result yet -- it might be something we were interested in a while ago\nbut that we've stopped being interested in. \n\n\nSo instead, what happens is the dependent latch will just clear itself and de-register its listener. And then if you ask it for its value again, it'll quickly make the call and re-register its listener.\n\n\nThis creates a \nlazy observer\n pattern: invalidations happen by the observer pattern, but calculations are always lazy (triggered by the next request for the value). And the listeners are only in-place if\nthere is a cached value that might need invalidating, so a cleared latch that goes out of scope can be garbage collected.\n\n\nflatMap takes a Future\n\n\nLatch.flatMap\n takes a function that returns a \nFuture\n, not a \nLatch\n. That is, it's defined as:\n\n\ndef flatMap[B](f: T =\n Future[B]):Latch[B]\n\n\n\n\nThis might seem odd to those that think of \"flatMap\" as being like \"bind\" in a monad. However, it makes a for comprehension using a Latch work in a sensible way: \n\n\nval result:Latch[C] = for { \n  a \n- latch\n  b \n- doSomethingWith(a)\n  c \n- doSomethingElseWith(b)\n} yield c\n\n\n\n\nresult\n (the result of the computation) is a latch. Whereas \ndoSomethingWith()\n are just asynchronous functions returning \nFuture[B]\n and \nFuture[C]\n.\n\n\nIf the original latch is cleared, \nresult\n will be invalidated. The intermediate steps (\nb\n and \nc\n) will be recomputed when \nresult\n is next asked for its value, because that's the way the computation is written, but they don't themselves need to be inside latches.\n\n\nIf you don't want the result to be a latch\n\n\nIn the example above, \nresult\n was a Latch. If you'd just like it to be a Future, just call \nlatch.request\n at the beginning of the for comprehension, so you're flatMapping over the Future instead of\nover the Latch itself.\n\n\ndef result:Future[C] = for { \n  a \n- latch.request\n  b \n- doSomethingWith(a)\n  c \n- doSomethingElseWith(b)\n} yield c\n\n\n\n\nUse in Scala.js with React.js\n\n\nThe place I find Latches particularly useful is when working with React.js in Scala.js programs. React regenerates the UI whenever any state in the program changes. This creates a very nice kind of UI-driven laziness:\n\n\nIn the model for our client-side program, we can keep some set of Latches that can cache variables of interest so we don't have to keep requesting them from the server. When any cached values are invalidated (updated or cleared), any dependent latches will be cleared automatically too. And if those variables are still on-screen (needed by the UI), then the UI rerender will trigger them to be recomputed.  \n\n\nTo help make this even easier, you can also register a listener that will be called when \nany\n Latch updates. Usually this is so that in React you can just say\n\n\nLatch.addGlobalListener(_ =\n rerender())", 
            "title": "Latch"
        }, 
        {
            "location": "/latch/#latch", 
            "text": "A  Latch  is a little wrapper around a Scala  Future  that makes it clearable. Think of it as a single item \nasynchronous cache. Latches can be dependent on one another, in what I call a  lazy observer  pattern.   Suppose we start with a variable for who my closest friend is:  val myClosestFriend = Latch.immediate(Algernon)  And we have an asynchronous calculation that is dependent on it -- in this case to see if my closest friend interacts most with me on a social network.  val closestFriendReciprocates:Latch[Boolean] = for { \n    friend  - myClosestFriend \n    hisClosest  - socialNetwork.getMostInteractedWithUser(friend)\n} yield (hisClosest == me)  We now have two latches that depend on each other. Suppose my closest friend changes for some reason.   myClosestFriend.fill(Bertie)  The variable on whether my closest friend reciprocates is now invalid. In an observer pattern, we might see that the closest friend has updated, and \nre-calculate the dependent value (make the call out to the social network). But actually, we might not need to use the result yet -- it might be something we were interested in a while ago\nbut that we've stopped being interested in.   So instead, what happens is the dependent latch will just clear itself and de-register its listener. And then if you ask it for its value again, it'll quickly make the call and re-register its listener.  This creates a  lazy observer  pattern: invalidations happen by the observer pattern, but calculations are always lazy (triggered by the next request for the value). And the listeners are only in-place if\nthere is a cached value that might need invalidating, so a cleared latch that goes out of scope can be garbage collected.", 
            "title": "Latch"
        }, 
        {
            "location": "/latch/#flatmap-takes-a-future", 
            "text": "Latch.flatMap  takes a function that returns a  Future , not a  Latch . That is, it's defined as:  def flatMap[B](f: T =  Future[B]):Latch[B]  This might seem odd to those that think of \"flatMap\" as being like \"bind\" in a monad. However, it makes a for comprehension using a Latch work in a sensible way:   val result:Latch[C] = for { \n  a  - latch\n  b  - doSomethingWith(a)\n  c  - doSomethingElseWith(b)\n} yield c  result  (the result of the computation) is a latch. Whereas  doSomethingWith()  are just asynchronous functions returning  Future[B]  and  Future[C] .  If the original latch is cleared,  result  will be invalidated. The intermediate steps ( b  and  c ) will be recomputed when  result  is next asked for its value, because that's the way the computation is written, but they don't themselves need to be inside latches.", 
            "title": "flatMap takes a Future"
        }, 
        {
            "location": "/latch/#if-you-dont-want-the-result-to-be-a-latch", 
            "text": "In the example above,  result  was a Latch. If you'd just like it to be a Future, just call  latch.request  at the beginning of the for comprehension, so you're flatMapping over the Future instead of\nover the Latch itself.  def result:Future[C] = for { \n  a  - latch.request\n  b  - doSomethingWith(a)\n  c  - doSomethingElseWith(b)\n} yield c", 
            "title": "If you don't want the result to be a latch"
        }, 
        {
            "location": "/latch/#use-in-scalajs-with-reactjs", 
            "text": "The place I find Latches particularly useful is when working with React.js in Scala.js programs. React regenerates the UI whenever any state in the program changes. This creates a very nice kind of UI-driven laziness:  In the model for our client-side program, we can keep some set of Latches that can cache variables of interest so we don't have to keep requesting them from the server. When any cached values are invalidated (updated or cleared), any dependent latches will be cleared automatically too. And if those variables are still on-screen (needed by the UI), then the UI rerender will trigger them to be recomputed.    To help make this even easier, you can also register a listener that will be called when  any  Latch updates. Usually this is so that in React you can just say  Latch.addGlobalListener(_ =  rerender())", 
            "title": "Use in Scala.js with React.js"
        }, 
        {
            "location": "/refcache/", 
            "text": "RefCache\n\n\nIf a \nLatch\n is a single-item asynchronous cache, a \nRefCache\n is a multi-item asynchronous cache.\n\n\nAgain, it contains listener methods so that on the client you can ensure that any invalidation of a cache entry will trigger a rerender.\n\n\nLookUpCache\n\n\nA \nLookUpCache\n is a particular kind of cache for looking up IDs. In future versions, it will use \nRefCache\n but at the moment it's its own thing.\n\n\nThere are a few unique features we would like a \nLookUpCache\n to be able to do:\n\n\n\n\nIf we give it a \nLazyId[T, K]\n we'd like it to see if it's already in the cache.\n\n\nIf it's given some other kind of \nRef[T]\n we'd like it to cache it against its ID.\n\n\nIf it's given a \nSeq[Id[T, K]]\n or an \nIds[T, K]\n we'd like it to be able to work out which ones are missing, bulk-fetch just those, and then return a \nRefMany[T]\n in the right order including the ones it had already cached.\n\n\n\n\nThat's not all in the current version, but will be in future ones.", 
            "title": "RefCache"
        }, 
        {
            "location": "/refcache/#refcache", 
            "text": "If a  Latch  is a single-item asynchronous cache, a  RefCache  is a multi-item asynchronous cache.  Again, it contains listener methods so that on the client you can ensure that any invalidation of a cache entry will trigger a rerender.", 
            "title": "RefCache"
        }, 
        {
            "location": "/refcache/#lookupcache", 
            "text": "A  LookUpCache  is a particular kind of cache for looking up IDs. In future versions, it will use  RefCache  but at the moment it's its own thing.  There are a few unique features we would like a  LookUpCache  to be able to do:   If we give it a  LazyId[T, K]  we'd like it to see if it's already in the cache.  If it's given some other kind of  Ref[T]  we'd like it to cache it against its ID.  If it's given a  Seq[Id[T, K]]  or an  Ids[T, K]  we'd like it to be able to work out which ones are missing, bulk-fetch just those, and then return a  RefMany[T]  in the right order including the ones it had already cached.   That's not all in the current version, but will be in future ones.", 
            "title": "LookUpCache"
        }, 
        {
            "location": "/id/", 
            "text": "Ids\n\n\nHandy includes a few classes for dealing with IDs of things, getting them, caching them etc. They are all fairly simple and unexciting. \n\n\nId[T, K]\n\n\nHandy includes a typed Id class: \nId[T, K]\n. \n\n\nSuppose you are writing an application that uses \nSnowflake\n for its IDs. You may wish to serialise those IDs always as Strings between the server and the client. But in your code, keeping it as an \nId[Person, String]\n has some advantages:\n\n\n\n\nIf you want to serialise them as a different type into the database -- for example, store them there as numbers, you can tell a string from a string-representing-an-ID by its type.\n\n\nAs the ID is also typed by what it is an ID for (this is a person's ID not a booking ID), the compiler can catch cases of assigning the wrong ID variable.\n\n\n\n\nIds[T, K]\n\n\nHandy also includes a class for a sequence of IDs: \nIds[T, K]\n. If you have a bunch of IDs you want to look up, you probably want to do them in batch rather than one at a time. \n\n\nConverting between \nSeq[Id[T,K]\n and \nIds[T, K]\n is fairly straightforward:\n\n\nimport Id._\nimport Ids._\n\nval seqIds:Seq[Id[Foo, Int]] = Seq(1.asId[Foo], 2.asId[Foo], 3.asId[Foo])\nval ids:Ids[Foo, Int] = seqIds.asIds\nids.toSeqId == seqIds\n\n\n\n\nLookUp\n\n\nA \nLookUp\n knows how to look up an \nId\n or \nIds\n. It is a very simple trait:\n\n\ntrait LookUp[T, -K] {\n\n  def one[KK \n: K](r:Id[T,KK]): Ref[T]\n\n  def many[KK \n: K](r:Ids[T,KK]): RefMany[T]\n\n}\n\n\n\n\nLazyId\n\n\nLazyId allows a very particular case of indecisiveness -- it lets you be indecisive about whether you want an object or just its ID.\n\n\nSuppose you wanted to describe a permissions check on whether a user can edit a page. Do you define that function to accept the page itself, or the ID of the page? It might be that when you start writing your app, your check is \"actually, any user can edit pages\", and later on you need to lock it down. \n\n\nWith handy, you just define your function as accepting a \nRef[Page]\n. A \nLazyId\n is a pair of the ID and a LookUp. So if the function needs to look up the item, it can. But if it just needs to pull out the ID, it happens immediately without fetching from the database.\n\n\ndef canEditPage(p:Ref[Page], u:Ref[User]):Ref[Boolean] = {\n  // An unusual permission rule, but if you've passed in a LazyId, this won't try to fetch the item\n  // but if you've passed in any other kind of Ref[Page] it'll still work\n  for {\n      pId \n- p.refId\n  } yield if (pId \n 1000) true else false   \n}\n\n\n\n\nThat might sound a bit obscure, but in the permissions system it turns out to be useful because\nsometimes you'll have rules like \"if they can edit the whole book, we don't need to fetch the page\nbecause they're allowed to edit anything within it\".\n\n\nrefId and GetsId\n\n\nSuppose we have a \nRef[User]\n and want to get its ID. Well, if it is a \nRefItself[User]\n (we have the user itself), we can get it from the \nUser\n object. And if it's a \nLazyId[User, _]\n and we haven't fetched the user at all yet, we can just get if from the LazyId. But what if it's a request in-flight -- a \nRefFuture[User]\n and the future hasn't completed yet? Well, we'll have to get it asynchronously when the \nFuture\n has completed. So, \nrefId\n returns \nRef[K]\n because it \nmight\n have to be asynchronous.\n\n\nWhat if we have a \nRef[String]\n and call \n.refId\n on it? Well, a String doesn't have an ID, so we'd rather make that a compile error because it doesn't make sense to do that. To tell the difference between things that have IDs and things that don't, \nrefId\n takes an implicit \nGetsId\n parameter that knows how to get the ID from the object. It has a fairly simple type:\n\n\ntrait GetsId[-T, K] {\n  def getId[TT \n: T](obj: TT): Option[Id[TT, K]]\n\n  def canonical[TT \n: T](o:Any):Option[Id[TT,K]]\n}\n\n\n\n\nThe \ncanonical\n method is needed because the compiler can't know if \nLazyId\n uses exactly the same key type that \nGetsId\n wants to produce. If the compiler is given a \nRef[T]\n there's no way for it to be sure that the \nK\ns in a \nLazyId[T, K]\n and a \nGetsId[T, K]\n will always be the same. They just usually are.\n\n\nIf your type implements \nHasId[T]\n, there will be one already implicitly in scope.", 
            "title": "Ids"
        }, 
        {
            "location": "/id/#ids", 
            "text": "Handy includes a few classes for dealing with IDs of things, getting them, caching them etc. They are all fairly simple and unexciting.", 
            "title": "Ids"
        }, 
        {
            "location": "/id/#idt-k", 
            "text": "Handy includes a typed Id class:  Id[T, K] .   Suppose you are writing an application that uses  Snowflake  for its IDs. You may wish to serialise those IDs always as Strings between the server and the client. But in your code, keeping it as an  Id[Person, String]  has some advantages:   If you want to serialise them as a different type into the database -- for example, store them there as numbers, you can tell a string from a string-representing-an-ID by its type.  As the ID is also typed by what it is an ID for (this is a person's ID not a booking ID), the compiler can catch cases of assigning the wrong ID variable.", 
            "title": "Id[T, K]"
        }, 
        {
            "location": "/id/#idst-k", 
            "text": "Handy also includes a class for a sequence of IDs:  Ids[T, K] . If you have a bunch of IDs you want to look up, you probably want to do them in batch rather than one at a time.   Converting between  Seq[Id[T,K]  and  Ids[T, K]  is fairly straightforward:  import Id._\nimport Ids._\n\nval seqIds:Seq[Id[Foo, Int]] = Seq(1.asId[Foo], 2.asId[Foo], 3.asId[Foo])\nval ids:Ids[Foo, Int] = seqIds.asIds\nids.toSeqId == seqIds", 
            "title": "Ids[T, K]"
        }, 
        {
            "location": "/id/#lookup", 
            "text": "A  LookUp  knows how to look up an  Id  or  Ids . It is a very simple trait:  trait LookUp[T, -K] {\n\n  def one[KK  : K](r:Id[T,KK]): Ref[T]\n\n  def many[KK  : K](r:Ids[T,KK]): RefMany[T]\n\n}", 
            "title": "LookUp"
        }, 
        {
            "location": "/id/#lazyid", 
            "text": "LazyId allows a very particular case of indecisiveness -- it lets you be indecisive about whether you want an object or just its ID.  Suppose you wanted to describe a permissions check on whether a user can edit a page. Do you define that function to accept the page itself, or the ID of the page? It might be that when you start writing your app, your check is \"actually, any user can edit pages\", and later on you need to lock it down.   With handy, you just define your function as accepting a  Ref[Page] . A  LazyId  is a pair of the ID and a LookUp. So if the function needs to look up the item, it can. But if it just needs to pull out the ID, it happens immediately without fetching from the database.  def canEditPage(p:Ref[Page], u:Ref[User]):Ref[Boolean] = {\n  // An unusual permission rule, but if you've passed in a LazyId, this won't try to fetch the item\n  // but if you've passed in any other kind of Ref[Page] it'll still work\n  for {\n      pId  - p.refId\n  } yield if (pId   1000) true else false   \n}  That might sound a bit obscure, but in the permissions system it turns out to be useful because\nsometimes you'll have rules like \"if they can edit the whole book, we don't need to fetch the page\nbecause they're allowed to edit anything within it\".", 
            "title": "LazyId"
        }, 
        {
            "location": "/id/#refid-and-getsid", 
            "text": "Suppose we have a  Ref[User]  and want to get its ID. Well, if it is a  RefItself[User]  (we have the user itself), we can get it from the  User  object. And if it's a  LazyId[User, _]  and we haven't fetched the user at all yet, we can just get if from the LazyId. But what if it's a request in-flight -- a  RefFuture[User]  and the future hasn't completed yet? Well, we'll have to get it asynchronously when the  Future  has completed. So,  refId  returns  Ref[K]  because it  might  have to be asynchronous.  What if we have a  Ref[String]  and call  .refId  on it? Well, a String doesn't have an ID, so we'd rather make that a compile error because it doesn't make sense to do that. To tell the difference between things that have IDs and things that don't,  refId  takes an implicit  GetsId  parameter that knows how to get the ID from the object. It has a fairly simple type:  trait GetsId[-T, K] {\n  def getId[TT  : T](obj: TT): Option[Id[TT, K]]\n\n  def canonical[TT  : T](o:Any):Option[Id[TT,K]]\n}  The  canonical  method is needed because the compiler can't know if  LazyId  uses exactly the same key type that  GetsId  wants to produce. If the compiler is given a  Ref[T]  there's no way for it to be sure that the  K s in a  LazyId[T, K]  and a  GetsId[T, K]  will always be the same. They just usually are.  If your type implements  HasId[T] , there will be one already implicitly in scope.", 
            "title": "refId and GetsId"
        }, 
        {
            "location": "/approval/", 
            "text": "Approval\n\n\nHandy includes a simple but surprisingly useful permissions system.\n\n\nImagine your request is a small schoolchild. And it needs to collect permission slips for what it wants to do. Sometimes, the teachers might look at the permission slips the child has already got, and say \"well, if you're allowed to play soccer, of course you're allowed to get a football\".\n\n\nApproval[U]\n works like a little wallet for permission slips. \nU\n is your user type, and it contains a Ref to its owner:\n\n\ncase class Approval[U](val who: Ref[U]) { \n    //...\n}\n\n\n\n\nPerm\n is a permission. It is a trait with a single method:\n\n\ntrait Perm[U] {\n  def resolve(prior: Approval[U]):Ref[Approved]\n}\n\n\n\n\nApproved\n is a type that represents a permission has been approved, and \nRefused\n is a Throwable to represent that something has not been approved. So:\n\n\nfor { \n    a \n- approval ask SomePermission \n} yield doSomething()\n\n\n\n\ndoSomething()\n will only occur if the permission is approved, and otherwise the result will be \nRefFailed(Refused(message))\n.\n\n\nprior\n\n\nThe \nresolve\n method takes a parameter called \nprior: Approval[U]\n so that the method (the permissions check) can use what permissions you've already been granted when making its determination.\n\n\ndef resolve(prior: Perm[User]) = {\n  // If the user is an administrator, we don't need to check any further\n  (prior ask CanAdminister) recoverWith { case Refused(_) =\n \n    // ... ok, they're not an admin so we ought to check...\n  } \n}\n\n\n\n\nAnd because we usually test permissions with\n\n\napproval ask permission\n\n\n\n\nand approval has a cache of permissions already granted, it will automatically check if it's already been granted a permission. The permissions wallet uses a ConcurrentHashMap, so entries can be invalidated. \n\n\nIf you want to check a permission without caching (for example, for time-sensitive checks on the client), there's two ways of doing that which are equivalent:\n\n\npermission.resolve(approval)\n\napproval askAfresh permission\n\n\n\n\nUsually on the server, the lifetime of an Approval (a permissions wallet) is short -- a single request, while on the client it may be longer (while the user is logged in).\n\n\nApprovals and asynchronicity\n\n\nThe return type of \nresolve\n is \nRef[Approved]\n. So it \nmight\n be asynchronous. This means your permissions check can also go and look things up. \"You may only edit your assignment submission if it is \nyour\n assignment submission and the due date for that assignment has not passed\". \n\n\ndef resolve(prior: Perm[User]) = for {\n    user       \n- prior.who\n    s          \n- submission if s.submittedBy == user.id\n                  orIfNone Refused(\nThis isn't your assignment\n)\n    assignment \n- s.assignmentId.lookUp(lu) if assignment.due \n getTime() \n                  orIfNone Refused(\nAssignment is past due\n)\n} yield Approved(\nYes you can edit it\n)\n\n\n\n\nPermission equality\n\n\nUsually, we want a permission's equality to be based around the ID of an object, not the object itself. \n\n\nConsider a request to the server to edit a user's profile.\n- The ID will remain the same\n- Some fields of the object will change\n\n\nSo in a request to edit a record it is usually the case that \nsubmittedRecord.id == originalRecord.id\n but \nsubmittedRecord != originalRecord\n. (Assuming you've written your business objects as case classes.) But we don't want \"permission to edit the submitted record\" and \"permission to edit the original record\" to be different items because conceptually they are permission to edit \nthat\n user's profile.\n\n\nPerm.onId\n\n\nIt is entirely reasonable for a permission to be a case class on an ID. For example:\n\n\ncase class CanEditCourse(id:Id[Course, String]) extends Perm[User] {\n  def resolve(p:Prior[User]) = ???\n}\n\n\n\n\nBut you might want it to work so that you can also pass in the \nCourse\n itself if you have it, so that \nresolve\n doesn't try to look it up from the database again. So, perhaps you would like your permission to do its equality-check based on the ID of an item, but be happy receiving any kind a \nRef\n, whether it's a \nLazyId\n or a \nRefItself\n or a \nRefFuture\n.\n\n\nHandy includes a little generator method \nPerm.onId[U, T, K]\n that is useful for these cases. \nU\n is your user type, \nT\n is the item type, and \nK\n is the key type. It requires an implicit \nGetsId[T, K]\n, because it needs to know how to extract the ID from the item.\n\n\n  val EditTask = Perm.onId[User, Task, String] { case (prior, refTask) =\n\n    for (\n        t \n- refTask;\n        a \n- prior ask EditCourse(t.course)\n    ) yield a\n  }\n\n\n\n\nAnd then \nEditTask(getUser(3)) == EditTask(user3.itself)", 
            "title": "Approval"
        }, 
        {
            "location": "/approval/#approval", 
            "text": "Handy includes a simple but surprisingly useful permissions system.  Imagine your request is a small schoolchild. And it needs to collect permission slips for what it wants to do. Sometimes, the teachers might look at the permission slips the child has already got, and say \"well, if you're allowed to play soccer, of course you're allowed to get a football\".  Approval[U]  works like a little wallet for permission slips.  U  is your user type, and it contains a Ref to its owner:  case class Approval[U](val who: Ref[U]) { \n    //...\n}  Perm  is a permission. It is a trait with a single method:  trait Perm[U] {\n  def resolve(prior: Approval[U]):Ref[Approved]\n}  Approved  is a type that represents a permission has been approved, and  Refused  is a Throwable to represent that something has not been approved. So:  for { \n    a  - approval ask SomePermission \n} yield doSomething()  doSomething()  will only occur if the permission is approved, and otherwise the result will be  RefFailed(Refused(message)) .", 
            "title": "Approval"
        }, 
        {
            "location": "/approval/#prior", 
            "text": "The  resolve  method takes a parameter called  prior: Approval[U]  so that the method (the permissions check) can use what permissions you've already been granted when making its determination.  def resolve(prior: Perm[User]) = {\n  // If the user is an administrator, we don't need to check any further\n  (prior ask CanAdminister) recoverWith { case Refused(_) =  \n    // ... ok, they're not an admin so we ought to check...\n  } \n}  And because we usually test permissions with  approval ask permission  and approval has a cache of permissions already granted, it will automatically check if it's already been granted a permission. The permissions wallet uses a ConcurrentHashMap, so entries can be invalidated.   If you want to check a permission without caching (for example, for time-sensitive checks on the client), there's two ways of doing that which are equivalent:  permission.resolve(approval)\n\napproval askAfresh permission  Usually on the server, the lifetime of an Approval (a permissions wallet) is short -- a single request, while on the client it may be longer (while the user is logged in).", 
            "title": "prior"
        }, 
        {
            "location": "/approval/#approvals-and-asynchronicity", 
            "text": "The return type of  resolve  is  Ref[Approved] . So it  might  be asynchronous. This means your permissions check can also go and look things up. \"You may only edit your assignment submission if it is  your  assignment submission and the due date for that assignment has not passed\".   def resolve(prior: Perm[User]) = for {\n    user        - prior.who\n    s           - submission if s.submittedBy == user.id\n                  orIfNone Refused( This isn't your assignment )\n    assignment  - s.assignmentId.lookUp(lu) if assignment.due   getTime() \n                  orIfNone Refused( Assignment is past due )\n} yield Approved( Yes you can edit it )", 
            "title": "Approvals and asynchronicity"
        }, 
        {
            "location": "/approval/#permission-equality", 
            "text": "Usually, we want a permission's equality to be based around the ID of an object, not the object itself.   Consider a request to the server to edit a user's profile.\n- The ID will remain the same\n- Some fields of the object will change  So in a request to edit a record it is usually the case that  submittedRecord.id == originalRecord.id  but  submittedRecord != originalRecord . (Assuming you've written your business objects as case classes.) But we don't want \"permission to edit the submitted record\" and \"permission to edit the original record\" to be different items because conceptually they are permission to edit  that  user's profile.", 
            "title": "Permission equality"
        }, 
        {
            "location": "/approval/#permonid", 
            "text": "It is entirely reasonable for a permission to be a case class on an ID. For example:  case class CanEditCourse(id:Id[Course, String]) extends Perm[User] {\n  def resolve(p:Prior[User]) = ???\n}  But you might want it to work so that you can also pass in the  Course  itself if you have it, so that  resolve  doesn't try to look it up from the database again. So, perhaps you would like your permission to do its equality-check based on the ID of an item, but be happy receiving any kind a  Ref , whether it's a  LazyId  or a  RefItself  or a  RefFuture .  Handy includes a little generator method  Perm.onId[U, T, K]  that is useful for these cases.  U  is your user type,  T  is the item type, and  K  is the key type. It requires an implicit  GetsId[T, K] , because it needs to know how to extract the ID from the item.    val EditTask = Perm.onId[User, Task, String] { case (prior, refTask) = \n    for (\n        t  - refTask;\n        a  - prior ask EditCourse(t.course)\n    ) yield a\n  }  And then  EditTask(getUser(3)) == EditTask(user3.itself)", 
            "title": "Perm.onId"
        }
    ]
}